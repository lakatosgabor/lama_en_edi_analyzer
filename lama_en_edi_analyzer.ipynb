{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakatosgabor/lama_en_edi_analyzer/blob/main/lama_en_edi_analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "924Sp5sCmllK"
      },
      "source": [
        "### Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkc5bwShc8Om"
      },
      "outputs": [],
      "source": [
        "!sudo apt install pciutils\n",
        "!pip install -q crewai\n",
        "!#curl -fsSL https://ollama.com/install.sh | sh\n",
        "#!pip install -qq pyngrok ollama\n",
        "#!pip install langchain\n",
        "#!pip install langchain_core\n",
        "!pip install faiss-cpu\n",
        "!pip install -q sentence-transformers\n",
        "!pip install deep-translator --quiet\n",
        "#!pip install langchain_community\n",
        "!pip install -U langchain-ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S40fQGWGfGPZ"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "import os\n",
        "os.environ[\"OLLAMA_NUM_GPU_LAYERS\"] = \"100\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr0Ksp0hfK4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04718c65-207c-4d24-f270-f0f5eea889af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A mesterséges intelligencia átalakítja a feldolgozóipart.\n"
          ]
        }
      ],
      "source": [
        "import IPython\n",
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "from typing import Any, Dict, List, Optional\n",
        "import requests\n",
        "import json\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "#from langchain_community.llms import Ollama\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import requests\n",
        "import faiss\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "from deep_translator import GoogleTranslator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_hungarian(text):\n",
        "    return GoogleTranslator(source='en', target='hu').translate(text)\n",
        "\n",
        "def retrieve_context(query, k=10):\n",
        "    query_vec = embed_model.encode([query])\n",
        "    D, I = index.search(query_vec, k)\n",
        "    return \"\\n\".join([chunks[i] for i in I[0]])"
      ],
      "metadata": {
        "id": "2ykez5KLre9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYA1eirq1Eda"
      },
      "source": [
        "### Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1knIbU7Kfa-b"
      },
      "outputs": [],
      "source": [
        "def start_ollama_server() -> None:\n",
        "    \"\"\"Starts the Ollama server.\"\"\"\n",
        "    subprocess.Popen(['ollama', 'serve'])\n",
        "    print(\"Ollama server started.\")\n",
        "\n",
        "\n",
        "def check_ollama_port(port: str) -> None:\n",
        "    \"\"\"Check if Ollama server is running at the specified port.\"\"\"\n",
        "    try:\n",
        "        subprocess.run(['sudo', 'lsof', '-i', '-P', '-n'], check=True, capture_output=True, text=True)\n",
        "        if any(f\":{port} (LISTEN)\" in line for line in subprocess.run(['sudo', 'lsof', '-i', '-P', '-n'], capture_output=True, text=True).stdout.splitlines()):\n",
        "            print(f\"Ollama is listening on port {port}\")\n",
        "        else:\n",
        "            print(f\"Ollama does not appear to be listening on port {port}.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "         print(f\"Error checking Ollama port: {e}\")\n",
        "\n",
        "\n",
        "def setup_ngrok_tunnel(port: str) -> ngrok.NgrokTunnel:\n",
        "    \"\"\"Sets up an ngrok tunnel.\n",
        "\n",
        "    Args:\n",
        "        port: The port to tunnel.\n",
        "\n",
        "    Returns:\n",
        "        The ngrok tunnel object.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the ngrok authtoken is not set.\n",
        "    \"\"\"\n",
        "    ngrok_auth_token = \"30PNgTX5ijqAbIicSn0ZHwEOOLU_3hQRpPf6wKHW25mTj462b\"\n",
        "    if not ngrok_auth_token:\n",
        "        raise RuntimeError(\"NGROK_AUTHTOKEN is not set.\")\n",
        "\n",
        "    ngrok.set_auth_token(ngrok_auth_token)\n",
        "    tunnel = ngrok.connect(port, host_header=f'localhost:{port}')\n",
        "    print(f\"ngrok tunnel created: {tunnel.public_url}\")\n",
        "    return tunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBHPpesQfgQp",
        "outputId": "efd290ba-b50f-4032-cfbc-dcdb64764861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok tunnel created: https://aae1e437f0c7.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "NGROK_PORT = '11434'\n",
        "ngrok_tunnel = setup_ngrok_tunnel(NGROK_PORT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiXwr0wYflk5",
        "outputId": "51ac8ba3-223d-453c-e5dc-c9a88c0b538f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama server started.\n",
            "Ollama does not appear to be listening on port 11434.\n"
          ]
        }
      ],
      "source": [
        "start_ollama_server()\n",
        "check_ollama_port(NGROK_PORT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQvvgWnzgJib",
        "outputId": "210544a9-46bc-4473-ca2a-ad4abc1b0ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "!ollama pull llama3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyA8lzPY1tGv"
      },
      "source": [
        "### Load LLAMA3 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3sBTUB7u4C4"
      },
      "outputs": [],
      "source": [
        "llm = OllamaLLM(\n",
        "    model=\"llama3\",\n",
        "    base_url=\"https://aae1e437f0c7.ngrok-free.app\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVdxAmCO10kf"
      },
      "source": [
        "### Testing LLAMA3 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7tWUjNkf2Ma",
        "outputId": "bc84ca7c-f80c-494e-e453-bbb5eea41de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not much! Just an AI hanging out, waiting to chat with you. How about you?"
          ]
        }
      ],
      "source": [
        "for chunk in llm.stream(\"What's up?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D78Ek4116rX"
      },
      "source": [
        "### Load and chunk dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdNIM5phu_rO"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/edi.csv\")\n",
        "print(df.head(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg3dr11Tx2M5",
        "outputId": "7309df37-f44a-4c0a-c8a7-84e80d613108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 7 chunk készült\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/edi.csv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "chunk_size = 500\n",
        "chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "print(f\"✅ {len(chunks)} chunk készült\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m718hU9P2J9w"
      },
      "source": [
        "### Create embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg1eKM0hyGhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a801016-638f-47f6-b03e-88e943a1611a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Embeddingek kész\n"
          ]
        }
      ],
      "source": [
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # all-mpnet-base-v2\n",
        "embeddings = embed_model.encode(chunks, convert_to_numpy=True)\n",
        "print(\"✅ Embeddingek kész\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sS_cBDDzNww",
        "outputId": "17ba7b28-a5be-4c5a-8f62-e7b43f29c017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FAISS index létrehozva\n"
          ]
        }
      ],
      "source": [
        "d = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(embeddings)\n",
        "print(\"✅ FAISS index létrehozva\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiFXNL_H2Phw"
      },
      "source": [
        "### Run the model, and  Analyze the data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZeX5iyzyltA"
      },
      "outputs": [],
      "source": [
        "query = \"List the orders where the customer modified the order quantity within 3 days.\"\n",
        "\n",
        "#query = \"List orders where the customer modified the order quantity within 3 days. Only include quantities that are divisible by 3.\"\n",
        "\n",
        "\n",
        "context = retrieve_context(query)\n",
        "\n",
        "prompt = f\"\"\"You are a customer contact. You are working on a dataset containing customer orders: {context}\n",
        "              Analyze the data set based on the question, then answer briefly and clearly. Consider the entire data set!\n",
        "              Question: {query}\n",
        "          \"\"\"\n",
        "\n",
        "answer_full_text=''\n",
        "for chunk in llm.stream(prompt):\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "    answer_full_text+=chunk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hungarian_text = translate_to_hungarian(answer_full_text)\n",
        "print(hungarian_text)"
      ],
      "metadata": {
        "id": "FRi33Xh2lXYI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPuvuQ3jMgLYbp5PfT+QaSp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}